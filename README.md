A SMALL LARGE LANGUAGE MODEL TRAINED ON 1.2LAKH TOKENS with 70M parameters
