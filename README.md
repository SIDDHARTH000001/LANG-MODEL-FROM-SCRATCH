# A Small Large Language Model

This repository contains the implementation of a small-scale large language model (LLM) with 70 million parameters, trained on 1.2 lakh tokens. The model is developed in Python and leverages the self-attention mechanism for efficient context understanding and text generation.

## Overview

This project demonstrates the core principles of building a transformer-based language model on a smaller scale. Despite its size, the model captures the essence of modern LLM architectures, making it an excellent educational tool and proof of concept for larger implementations.

### Key Features
- **Transformer Architecture:**
  - Self-attention mechanism for sequence modeling.
  - Multi-head attention and feedforward layers.
  - Layer normalization and residual connections.

- **Training Dataset:**
  - A corpus containing 1.2 lakh tokens, curated to provide diverse linguistic contexts.

- **70M Parameters:**
  - Designed to balance complexity and computational feasibility.

- **Python Implementation:**
  - Easy to understand and extend.
  - Uses popular libraries like NumPy and PyTorch.



## Getting Started

### Prerequisites
- Python 3.11+
- Virtual environment (optional, recommended)



## Future Work
- Scale the model to larger datasets and parameter sizes.
- Implement optimization techniques for faster training.
- Integrate with Hugging Face for easier deployment.

## Contributions
Contributions are welcome! Feel free to open issues or submit pull requests for improvements or new features.

## License
This project is licensed under the MIT License. See the LICENSE file for details.

## Acknowledgments
- Inspired by foundational transformer research papers.
- Special thanks to open-source resources and libraries for enabling this project.
